# AI-Powered Document Processing System 
### **ğŸ“„ Project Summary**  
Demo Link: https://youtu.be/EA8dEzMJPnM

ğŸš€ **Extract, Standardize, and Store Data from PDFs and Webpages**  

## **ğŸ“Œ Overview**
This project is an **AI-powered document processing system** that extracts, standardizes, and stores data from **PDFs and webpages**. It uses **open-source tools** (PyPDF2, pdfplumber, BeautifulSoup) and an **enterprise service** (Microsoft Document Intelligence) to process unstructured data. Extracted content is standardized into **Markdown format** using **Docling and MarkItDown** and stored in **AWS S3** for retrieval.  

The system provides:  
âœ… **FastAPI backend** for document processing  
âœ… **Streamlit web interface** for user-friendly interaction  
âœ… **Cloud storage (AWS S3)** for processed files  

---

## **ğŸ”‘ Features**
âœ… **Extract text, images, charts, and tables** from PDFs & webpages  
âœ… **Use AI-powered document processing** (Microsoft Document Intelligence)  
âœ… **Compare Open-Source vs. Enterprise tools** for text extraction  
âœ… **Standardize extracted content** into Markdown using **Docling & MarkItDown**  
âœ… **Store processed files in AWS S3** for easy retrieval  
âœ… **Provide an API with FastAPI** for seamless integration  
âœ… **User-friendly Streamlit Web App** to upload and process files  

---

## **âœ” Technology Stack**

| **Category**       | **Tools Used** |
|------------------|--------------|
| **Programming Language** | Python 3.8+ |
| **PDF Processing** | PyPDF2, pdfplumber, Microsoft Document Intelligence |
| **Web Scraping** | BeautifulSoup, requests |
| **Markdown Standardization** | Docling, MarkItDown |
| **Backend API** | FastAPI |
| **Frontend UI** | Streamlit |
| **Cloud Storage** | AWS S3 |
| **Deployment** | Render, Streamlit Cloud |

---

## **ğŸ› ï¸ Diagrams**

![image](./docs/data_extraction_architecture.png)


---

## **ğŸ› ï¸ User Guide**
1. Users can choose different ways to extract data. 
2. When user inputs a PDF or URL, it will be temporarily stored in S3 first
3. PDF data will be processed through the API call function to obtain the table image and text
4. Text will be marked down in two ways (docling markitdown)
5. Finally a **download link** will be returned, which contains all the output files.

---

## **ğŸ“‚ Project Structure**
```plaintext
â”œ.
â”œâ”€â”€ .github
â”‚   â””â”€â”€ workflows          # CI/CD automation setup
â”œâ”€â”€ docs
â”‚   â””â”€â”€ Codelab.md         # Project documentation
â”œâ”€â”€ webapp
â”‚   â”œâ”€â”€ frontend           # Streamlit frontend for user interaction
â”‚   â”‚   â””â”€â”€ src
â”‚   â”‚       â””â”€â”€ main.py    # Frontend entry point
â”‚   â””â”€â”€ backend
â”‚       â””â”€â”€ src
â”‚           â”œâ”€â”€ S3
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â””â”€â”€ s3_organization.py
â”‚           â”œâ”€â”€ api           # FastAPI backend
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â””â”€â”€ main.py   # FastAPI entry point
â”‚           â”œâ”€â”€ extraction    # Extract data from PDFs & webpages
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â”œâ”€â”€ pdf_parser_enterprise.py    # PDF extraction using Microsoft Document Intelligence
â”‚           â”‚   â”œâ”€â”€ pdf_parser_opensource.py    # PDF extraction using PyPDF2, pdfplumber
â”‚           â”‚   â””â”€â”€ web_scraper.py              # Web scraping using BeautifulSoup
â”‚           â””â”€â”€ standardization                 # Standardization using Docling & MarkItDown
â”‚               â”œâ”€â”€ __init__.py
â”‚               â”œâ”€â”€ docling_utils.py
â”‚               â””â”€â”€ markitdown_utils.py
â”œâ”€â”€ .gitignore            # Git ignore file
â”œâ”€â”€ AiuseDisclosure.md    # AI usage disclosure
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ ToolsComparison.md    # Tools comparison documentation

```

---

## **ğŸš€ Installation & Setup**
1ï¸âƒ£ Prerequisites
Ensure you have:

Python 3.8 or higher
pip (Python package manager)
AWS credentials (if storing files in AWS S3)

2ï¸âƒ£ Clone the Repository
```
git clone https://github.com/Neneanuj/BigData_Labs.git
cd BigData_Labs
```

3ï¸âƒ£ Create a Virtual Environment
```
python -m venv env
source env/bin/activate  # macOS/Linux
env\Scripts\activate     # Windows
```
---

## **ğŸ› ï¸ Usage**

1ï¸âƒ£ Run the FastAPI Backend
```
uvicorn src.api.main:app --reload
```
API will be available at:
ğŸ”— http://127.0.0.1:8000/docs

2ï¸âƒ£ Run the Streamlit App
```
streamlit run src/streamlit_app/app.py
```

App will open at:
ğŸ”— http://localhost:8501

3ï¸âƒ£ Upload a File or Webpage
* Upload a PDF file or enter a webpage URL in the Streamlit app.
* The API processes the document and returns standardized Markdown output.
* Extracted content is stored in AWS S3 (if enabled).

---

## **ğŸ“Œ Expected Outcomes**

* A functional AI-powered document processing system for extracting and standardizing data.
* A working API & Streamlit app that allow users to process PDFs & webpages.  
* A performance comparison between open-source and enterprise extraction tools.  
* A fully documented project with findings, code, and usage guidelines.  

---

## **ğŸ“Œ AI Use Disclosure**
This project uses:

* Microsoft Document Intelligence for enterprise PDF processing.
* Docling & MarkItDown for text standardization.
* AWS S3 for cloud storage.
ğŸ“„ See AiUseDisclosure.md for details.

---

## **ğŸ‘¨â€ğŸ’» Authors**
* Sicheng Bao (@Jellysillyfish13)
* Yung Rou Ko (@KoYungRou)
* Anuj Rajendraprasad Nene (@Neneanuj)

---

## **ğŸ“ Contact**
For questions, reach out via Big Data Course or open an issue on GitHub.
